
# ğŸŒŒ Multimodal Sentiment Analysis

## ğŸª„ What This Project Is Really About

Human emotions are messy, but our sensors capture quiet clues.
This project pulls those threads together to **classify sentiment**â€”**Positive (1), Neutral (0), Negative (2)**â€”
by combining **four very different streams of evidence**:

| Signal                     | Why We Care                                    | Sample Nuggets                              |
| -------------------------- | ---------------------------------------------- | ------------------------------------------- |
| ğŸ§  **EEG**                 | Brain rhythms hint at engagement and mood      | Deltaâ€“Gamma bandpower, frontal asymmetry    |
| ğŸŒŠ **GSR**                 | Tiny shifts in skin conductance reveal arousal | Mean level, number of SCR peaks             |
| ğŸ™‚ **Facial Action Units** | Micro-expressions whisper feelings             | AU12 smile, AU4 frown, AU1 brow raise       |
| âœï¸ **Self-Reports**        | The participantâ€™s own voice                    | Valence/arousal ratings mapped to sentiment |

Instead of betting on a single sensor, we **fuse them late**, stack their strengths, and
experiment with **transformer architectures** that let each modality â€œtalkâ€ to the others.

---

## ğŸ”§ Key Moves

* **Time-Aligned Preprocessing** â€“ EEG, GSR, and facial features synced to task events.
* **Feature Crafting** â€“ From engagement indices to AU-based valence estimates.
* **Model Line-Up**

  * Classic baselines: Logistic Regression, Random Forest, XGBoost
  * Late-fusion ensembles: voting, stacking, weighted logits
  * Multimodal Transformers: cross-modal attention for deep fusion
* **Transparent Evaluation** â€“ Macro-F1, Cohenâ€™s Kappa, SHAP plots, attention heatmaps.

---

## ğŸ—‚ Folder Map

```
project/
â”œâ”€ data/                  
â”œâ”€ notebooks/              
â”‚   â”œâ”€ 01_preprocessing.ipynb
â”‚   â”œâ”€ 02_feature_engineering.ipynb
â”‚   â”œâ”€ 03_modeling_baseline_sentiment.ipynb
â”‚   â”œâ”€ 04_modeling_fusion.ipynb
â”‚   â”œâ”€ 05_modeling_transformers.ipynb
â”‚   â””â”€ 06_analysis.ipynb
â”œâ”€ models/                 
â””â”€ README.md
```

---

## âš¡ Quick Start

1. **Clone the repo**

   ```bash
   git clone https://github.com/<your-username>/multimodal-sentiment-analysis.git
   cd multimodal-sentiment-analysis/project
   ```
2. **Open the notebooks**
   Each notebook is self-contained with its own imports.
   Run them in order to walk through preprocessing â†’ modeling â†’ analysis.
   *No global `requirements.txt` is required.*

---

## ğŸ§ª Things to Try

* **Sensor Mix-and-Match** â€“ Drop one modality at a time to see its real impact.
* **Binary vs. Ternary** â€“ Compare Positive/Negative only vs. full 3-class setup.
* **Temporal Smoothing** â€“ Track sentiment drift across a session.
* **Creative Augmentation** â€“ GAN-generated EEG, jittered GSR, synthetic facial AUs.

---

## ğŸŒŸ Why This Repository Feels Different

* Blends **neuroscience, physiology, and behavior** in a single, reproducible pipeline.
* Clear, modular notebooks that read more like a **guided lab journal** than boilerplate code.
* Designed for curious researchers who want to **peek under the hood of human affect**.

---

