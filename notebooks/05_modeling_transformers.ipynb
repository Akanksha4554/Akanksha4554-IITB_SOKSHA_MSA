{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "STEP 5: Modeling Transformers"
      ],
      "metadata": {
        "id": "MQPgyO3tmuXL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "xWlg-kUerZ_J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b8e0d44c"
      },
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
        "        super().__init__()\n",
        "        self.att = nn.MultiheadAttention(embed_dim, num_heads, dropout=rate, batch_first=True)\n",
        "        self.ffn = nn.Sequential(\n",
        "            nn.Linear(embed_dim, ff_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(ff_dim, embed_dim),\n",
        "        )\n",
        "        self.layernorm1 = nn.LayerNorm(embed_dim, eps=1e-6)\n",
        "        self.layernorm2 = nn.LayerNorm(embed_dim, eps=1e-6)\n",
        "        self.dropout1 = nn.Dropout(rate)\n",
        "        self.dropout2 = nn.Dropout(rate)\n",
        "\n",
        "    def forward(self, x):\n",
        "        attn_output, _ = self.att(x, x, x)\n",
        "        x = self.layernorm1(x + self.dropout1(attn_output))\n",
        "        ffn_output = self.ffn(x)\n",
        "        return self.layernorm2(x + self.dropout2(ffn_output))\n",
        "\n",
        "class TransformerClassifier(nn.Module):\n",
        "    def __init__(self, input_dim, embed_dim, num_heads, ff_dim, num_layers, num_classes, rate=0.1):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Linear(input_dim, embed_dim)\n",
        "        self.transformer_blocks = nn.Sequential(*[\n",
        "            TransformerBlock(embed_dim, num_heads, ff_dim, rate) for _ in range(num_layers)\n",
        "        ])\n",
        "        self.global_avg_pool = nn.AdaptiveAvgPool1d(1) # Pool over sequence length\n",
        "        self.classifier = nn.Linear(embed_dim, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        # Transformer expects shape (batch_size, seq_len, embed_dim).\n",
        "        # Here, we treat each feature as a step in a sequence of length = number of features\n",
        "        x = x.unsqueeze(1) # Add sequence length dimension (1)\n",
        "        x = self.transformer_blocks(x)\n",
        "        x = x.squeeze(1) # Remove sequence length dimension (1) after blocks\n",
        "        # Global average pooling\n",
        "        x = x.unsqueeze(-1) # Add a dimension for AdaptiveAvgPool1d\n",
        "        x = self.global_avg_pool(x)\n",
        "        x = x.squeeze(-1)\n",
        "        return self.classifier(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4338cc0d",
        "outputId": "760d9b30-ac14-490c-d638-64f0f6f9e459"
      },
      "source": [
        "# Create dataset and dataloaders\n",
        "# Combine all features for transformer input\n",
        "X_train_combined = np.hstack([X_eeg_train_scaled, X_gsr_train_scaled, X_tiva_train_scaled, X_sr_train_scaled])\n",
        "X_test_combined  = np.hstack([X_eeg_test_scaled, X_gsr_test_scaled, X_tiva_test_scaled, X_sr_test_scaled])\n",
        "\n",
        "train_dataset_tf = TensorDataset(torch.tensor(X_train_combined, dtype=torch.float32), torch.tensor(y_train, dtype=torch.long))\n",
        "test_dataset_tf  = TensorDataset(torch.tensor(X_test_combined, dtype=torch.float32), torch.tensor(y_test, dtype=torch.long))\n",
        "\n",
        "train_loader_tf  = DataLoader(train_dataset_tf, batch_size=32, shuffle=True)\n",
        "test_loader_tf   = DataLoader(test_dataset_tf, batch_size=32, shuffle=False)\n",
        "\n",
        "print(\"Transformer train data shape:\", X_train_combined.shape)\n",
        "print(\"Transformer test data shape:\", X_test_combined.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transformer train data shape: (3939, 42)\n",
            "Transformer test data shape: (985, 42)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "20b6b583",
        "outputId": "14bdf487-1a5b-4f4d-9a32-ead85fb6bc17"
      },
      "source": [
        "# Transformer model training\n",
        "input_dim = X_train_combined.shape[1]\n",
        "embed_dim = 64 # Embedding dimension for features\n",
        "num_heads = 4  # Number of attention heads\n",
        "ff_dim = 128   # Hidden layer size in feedforward network inside transformer\n",
        "num_layers = 2 # Number of transformer blocks\n",
        "num_classes = 3 # Number of sentiment classes\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "transformer_model = TransformerClassifier(input_dim, embed_dim, num_heads, ff_dim, num_layers, num_classes).to(device)\n",
        "\n",
        "criterion_tf = nn.CrossEntropyLoss()\n",
        "optimizer_tf = optim.Adam(transformer_model.parameters(), lr=0.001)\n",
        "\n",
        "epochs_tf = 50\n",
        "for epoch in range(epochs_tf):\n",
        "    transformer_model.train()\n",
        "    total_loss = 0\n",
        "    for xb, yb in train_loader_tf:\n",
        "        xb, yb = xb.to(device), yb.to(device)\n",
        "        optimizer_tf.zero_grad()\n",
        "        out = transformer_model(xb)\n",
        "        loss = criterion_tf(out, yb)\n",
        "        loss.backward()\n",
        "        optimizer_tf.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    # Evaluate on test set\n",
        "    transformer_model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    with torch.no_grad():\n",
        "        for xb, yb in test_loader_tf:\n",
        "            xb, yb = xb.to(device), yb.to(device)\n",
        "            outputs = transformer_model(xb)\n",
        "            _, preds = torch.max(outputs.data, 1)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(yb.cpu().numpy())\n",
        "\n",
        "    test_accuracy = accuracy_score(all_labels, all_preds)\n",
        "    test_macro_f1 = f1_score(all_labels, all_preds, average='macro')\n",
        "\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        print(f\"Epoch {epoch+1}/{epochs_tf}, Train Loss: {total_loss/len(train_loader_tf):.4f}, Test Accuracy: {test_accuracy:.4f}, Test Macro F1: {test_macro_f1:.4f}\")\n",
        "\n",
        "print(\"\\nTransformer Model Evaluation:\")\n",
        "print(classification_report(all_labels, all_preds))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10/50, Train Loss: 0.6370, Test Accuracy: 0.7005, Test Macro F1: 0.3494\n",
            "Epoch 20/50, Train Loss: 0.5307, Test Accuracy: 0.6579, Test Macro F1: 0.3943\n",
            "Epoch 30/50, Train Loss: 0.3970, Test Accuracy: 0.6518, Test Macro F1: 0.4126\n",
            "Epoch 40/50, Train Loss: 0.2708, Test Accuracy: 0.6284, Test Macro F1: 0.4206\n",
            "Epoch 50/50, Train Loss: 0.2022, Test Accuracy: 0.6396, Test Macro F1: 0.4155\n",
            "\n",
            "Transformer Model Evaluation:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.12      0.07      0.09        43\n",
            "           1       0.74      0.76      0.75       684\n",
            "           2       0.41      0.40      0.40       258\n",
            "\n",
            "    accuracy                           0.64       985\n",
            "   macro avg       0.42      0.41      0.42       985\n",
            "weighted avg       0.63      0.64      0.63       985\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c896c47d",
        "outputId": "d5ac3b70-43bc-465c-c02f-f28641b1806c"
      },
      "source": [
        "# Save the trained Transformer model\n",
        "transformer_model_path = \"models/transformer__model.pt\"\n",
        "torch.save(transformer_model.state_dict(), transformer_model_path)\n",
        "print(\"Saved Transformer model to:\", transformer_model_path)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved Transformer model to: models/transformer__model.pt\n"
          ]
        }
      ]
    }
  ]
}